{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MNAXxXB3qehB"
      },
      "outputs": [],
      "source": [
        "import os, json, math, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, brier_score_loss, roc_curve\n",
        "from scipy.stats import chi2\n",
        "\n",
        "#  PATHS\n",
        "DATA_DIR    = Path(\"/content/drive/MyDrive/freddie mac\")\n",
        "FEAT_DIR    = DATA_DIR / \"features\"\n",
        "MODELS_DIR  = DATA_DIR / \"models\"\n",
        "\n",
        "PD_FEATURES_FILE  = FEAT_DIR / \"features_pd.parquet\"\n",
        "META_FILE         = FEAT_DIR / \"feature_meta.json\"\n",
        "\n",
        "# Preferred PD model (will gracefully fall back if missing)\n",
        "PREFERRED_PD_FILES = [\n",
        "    MODELS_DIR / \"pd_model_xgb_isotonic.joblib\",    # preferred\n",
        "    MODELS_DIR / \"pd_model_rf_cal.joblib\",          # fallback\n",
        "    MODELS_DIR / \"pd_model_logit_calibrated.joblib\" # fallback\n",
        "]\n",
        "\n",
        "# LGD models + calibrators (origination-only)\n",
        "LGD_MODELS = {\n",
        "    \"xgb\":  MODELS_DIR / \"lgd_model_xgb.joblib\",\n",
        "    \"rf\":   MODELS_DIR / \"lgd_model_rf.joblib\",\n",
        "    \"ridge\":MODELS_DIR / \"lgd_model_logit_ols.joblib\"  # dict {'ridge':..., 'num_pipe':...}\n",
        "}\n",
        "LGD_CAL   = {\n",
        "    \"xgb\":   MODELS_DIR / \"lgd_calibrator_xgb_orig_only.joblib\",\n",
        "    \"rf\":    MODELS_DIR / \"lgd_calibrator_rf_orig_only.joblib\",\n",
        "    \"ridge\": MODELS_DIR / \"lgd_calibrator_ridge_orig_only.joblib\"\n",
        "}\n",
        "\n",
        "# EAD/CCF models + calibrators\n",
        "EAD_MODELS = {\n",
        "    \"xgb\":   MODELS_DIR / \"ead_model_xgb.joblib\",\n",
        "    \"rf\":    MODELS_DIR / \"ead_model_rf.joblib\",\n",
        "    \"ridge\": MODELS_DIR / \"ead_model_ridge.joblib\"  # dict {'ridge':..., 'num_pipe':...}\n",
        "}\n",
        "EAD_CAL = {\n",
        "    \"xgb\":   MODELS_DIR / \"ead_ccf_calibrator_xgb.joblib\",\n",
        "    \"rf\":    MODELS_DIR / \"ead_ccf_calibrator_rf.joblib\",\n",
        "    \"ridge\": MODELS_DIR / \"ead_ccf_calibrator_ridge.joblib\"\n",
        "}\n",
        "\n",
        "# Monte-Carlo settings\n",
        "N_DRAWS     = 3000           # portfolio simulations\n",
        "CHUNK_SIZE  = 120_000        # score/MC in chunks to control memory\n",
        "RNG_SEED    = 42\n",
        "\n",
        "# Stress scenario multipliers (EBA-style, simple scalars)\n",
        "SCENARIOS = {\n",
        "    \"Baseline\": {\"pd_mult\": 1.00, \"lgd_mult\": 1.00, \"ccf_mult\": 1.00},\n",
        "    \"Adverse\":  {\"pd_mult\": 1.50, \"lgd_mult\": 1.10, \"ccf_mult\": 1.05},\n",
        "    \"Severe\":   {\"pd_mult\": 2.50, \"lgd_mult\": 1.20, \"ccf_mult\": 1.10},\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load portfolio feature table (one row per loan)\n",
        "pd_df = pd.read_parquet(PD_FEATURES_FILE)\n",
        "print(\"Rows:\", len(pd_df))\n",
        "print(\"Columns sample:\", pd_df.columns.tolist()[:25])\n",
        "\n",
        "# Identify feature columns used during training\n",
        "# numeric + WoE (adjust automatically to what's present)\n",
        "NUM_CANDS = [\n",
        "    'credit_score','original_debt_to_income_dti_ratio',\n",
        "    'original_loan_to_value_ltv','original_combined_loan_to_value_cltv',\n",
        "    'original_interest_rate','original_upb','orig_upb' # one of these will exist\n",
        "]\n",
        "WOE_CANDS = [\n",
        "    'channel_woe','occupancy_status_woe','property_type_woe',\n",
        "    'loan_purpose_woe','property_state_woe'\n",
        "]\n",
        "\n",
        "X_cols = [c for c in NUM_CANDS+WOE_CANDS if c in pd_df.columns]\n",
        "# prefer 'original_upb' over 'orig_upb' if both exist\n",
        "if 'original_upb' in X_cols and 'orig_upb' in X_cols: X_cols.remove('orig_upb')\n",
        "\n",
        "assert len(X_cols)>0, \"No model features found in features_pd.parquet!\"\n",
        "\n",
        "# Determine original UPB column for EAD back-transform\n",
        "ORIG_UPB_COL = 'original_upb' if 'original_upb' in pd_df.columns else ('orig_upb' if 'orig_upb' in pd_df.columns else None)\n",
        "assert ORIG_UPB_COL is not None, \"Need original_upb/orig_upb in features file.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbwDxU72qu17",
        "outputId": "298069f7-24ea-47b3-bf6f-280f2997e477"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 737500\n",
            "Columns sample: ['loan_sequence_number', 'original_upb', 'credit_score', 'original_debt_to_income_dti_ratio', 'original_loan_to_value_ltv', 'original_combined_loan_to_value_cltv', 'original_interest_rate', 'channel', 'occupancy_status', 'property_type', 'loan_purpose', 'property_state', 'first_payment_date', 'maturity_date', 'pd_default_flag', '__first_90dpd_month', '__first_liq_month', 'dti_bin', 'ltv_bin', 'channel_woe', 'occupancy_status_woe', 'property_type_woe', 'loan_purpose_woe', 'property_state_woe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ks_stat(y_true, p):\n",
        "    fpr, tpr, thr = roc_curve(y_true, p)\n",
        "    return float((tpr - fpr).max())\n",
        "\n",
        "def hosmer_lemeshow(y_true, p_hat, bins=10):\n",
        "    # equal-count bins on predicted probability\n",
        "    df = pd.DataFrame({\"y\": y_true.astype(int), \"p\": p_hat}).sort_values(\"p\")\n",
        "    df[\"bin\"] = pd.qcut(df[\"p\"], bins, duplicates=\"drop\")\n",
        "    g = df.groupby(\"bin\").agg(n=(\"y\",\"size\"), obs=(\"y\",\"sum\"), pred=(\"p\",\"sum\")).reset_index(drop=True)\n",
        "    g[\"exp\"] = g[\"pred\"]\n",
        "    g[\"obs_non\"] = g[\"n\"] - g[\"obs\"]\n",
        "    g[\"exp_non\"] = g[\"n\"] - g[\"exp\"]\n",
        "    # HL statistic\n",
        "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "        part1 = (g[\"obs\"] - g[\"exp\"])**2 / (g[\"exp\"] + 1e-12)\n",
        "        part2 = (g[\"obs_non\"] - g[\"exp_non\"])**2 / (g[\"exp_non\"] + 1e-12)\n",
        "    hl = float((part1 + part2).sum())\n",
        "    dfreedom = 8  # bins-2\n",
        "    pval = 1 - chi2.cdf(hl, dfreedom)\n",
        "    return hl, pval, g\n",
        "\n",
        "def print_pd_metrics(y_true, proba, name=\"PD\"):\n",
        "    auc = roc_auc_score(y_true, proba)\n",
        "    ks  = ks_stat(y_true, proba)\n",
        "    brier = brier_score_loss(y_true, proba)\n",
        "    hl, pval, _ = hosmer_lemeshow(y_true, proba, bins=10)\n",
        "    print(f\"[{name}] AUC={auc:.4f} | KS={ks:.4f} | HL={hl:,.2f} (p={pval:.3f}) | Brier={brier:.5f}\")\n",
        "\n",
        "def load_first_existing(paths):\n",
        "    for p in paths:\n",
        "        if Path(p).exists():\n",
        "            return joblib.load(p), Path(p).name\n",
        "    raise FileNotFoundError(f\"None of the PD model files exist: {paths}\")\n"
      ],
      "metadata": {
        "id": "eWVcAOrgrKLJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  PD (pick the first available calibrated model, preferring XGB-isotonic)\n",
        "pd_model, pd_model_name = load_first_existing(PREFERRED_PD_FILES)\n",
        "print(\"Loaded PD model:\", pd_model_name)\n",
        "\n",
        "#  LGD (origination-only)\n",
        "lgd_loaded = {}\n",
        "lgd_cal    = {}\n",
        "for k, path in LGD_MODELS.items():\n",
        "    if path.exists():\n",
        "        lgd_loaded[k] = joblib.load(path)\n",
        "for k, path in LGD_CAL.items():\n",
        "    if path.exists():\n",
        "        lgd_cal[k] = joblib.load(path)\n",
        "print(\"LGD models loaded:\", list(lgd_loaded.keys()))\n",
        "print(\"LGD calibrators loaded:\", list(lgd_cal.keys()))\n",
        "\n",
        "#  EAD / CCF\n",
        "ead_loaded = {}\n",
        "ead_cal    = {}\n",
        "for k, path in EAD_MODELS.items():\n",
        "    if path.exists():\n",
        "        ead_loaded[k] = joblib.load(path)\n",
        "for k, path in EAD_CAL.items():\n",
        "    if path.exists():\n",
        "        ead_cal[k] = joblib.load(path)\n",
        "print(\"EAD models loaded:\", list(ead_loaded.keys()))\n",
        "print(\"EAD calibrators loaded:\", list(ead_cal.keys()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHv0ZyIJrN-O",
        "outputId": "b9e9e028-0322-420f-e884-154070c321f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded PD model: pd_model_xgb_isotonic.joblib\n",
            "LGD models loaded: ['xgb', 'rf', 'ridge']\n",
            "LGD calibrators loaded: ['xgb', 'rf', 'ridge']\n",
            "EAD models loaded: ['xgb', 'rf', 'ridge']\n",
            "EAD calibrators loaded: ['xgb', 'rf', 'ridge']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model input X\n",
        "X = pd_df[X_cols].copy()\n",
        "for c in X.columns:\n",
        "    X[c] = pd.to_numeric(X[c], errors='coerce')\n",
        "\n",
        "\n",
        "#  Align X columns to exactly what the saved PD model expects\n",
        "def _unwrap_estimator(est):\n",
        "    # peel calibrated/grid/pipeline wrappers to reach the actual classifier\n",
        "    if hasattr(est, \"calibrated_classifiers_\"):          # CalibratedClassifierCV\n",
        "        return _unwrap_estimator(est.calibrated_classifiers_[0].estimator)\n",
        "    if hasattr(est, \"best_estimator_\"):                  # GridSearchCV\n",
        "        return _unwrap_estimator(est.best_estimator_)\n",
        "    if hasattr(est, \"steps\"):                            # Pipeline\n",
        "        return _unwrap_estimator(est.steps[-1][1])\n",
        "    return est\n",
        "\n",
        "def _get_expected_feature_names(est):\n",
        "    base = _unwrap_estimator(est)\n",
        "    # sklearn-compatible\n",
        "    if hasattr(base, \"feature_names_in_\"):\n",
        "        return list(base.feature_names_in_)\n",
        "    # xgboost booster stores names if fitted on a DataFrame\n",
        "    try:\n",
        "        if hasattr(base, \"get_booster\"):\n",
        "            bn = base.get_booster().feature_names\n",
        "            if bn is not None:\n",
        "                return list(bn)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "expected = _get_expected_feature_names(pd_model)\n",
        "if expected is not None:\n",
        "    missing = [c for c in expected if c not in X.columns]\n",
        "    extra   = [c for c in X.columns   if c not in expected]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Features required by the PD model are missing from your table: {missing}\")\n",
        "    if extra:\n",
        "        print(\"Dropping extra columns not used in PD training:\", extra)\n",
        "    # reorder & drop extras to match training exactly\n",
        "    X = X[expected]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Predict PD (pd_model is already a pipeline or a calibrated wrapper)\n",
        "pd_hat = pd_model.predict_proba(X)[:,1]\n",
        "pd_df['PD'] = np.clip(pd_hat, 1e-6, 1-1e-6)\n",
        "\n",
        "# If labels exist, re-create the time split and validate\n",
        "def make_splits(df, date_cols=('first_payment_date','orig_month','__first_default_month'),\n",
        "                train_end='2018-12-01', valid_end='2021-12-01'):\n",
        "    date_col = next((c for c in date_cols if c in df.columns), None)\n",
        "    if date_col is None:\n",
        "        r = np.random.RandomState(42).rand(len(df))\n",
        "        return (r<0.6), ((r>=0.6)&(r<0.8)), (r>=0.8), None\n",
        "    d = pd.to_datetime(df[date_col], errors='coerce')\n",
        "    train = d <= pd.to_datetime(train_end)\n",
        "    valid = (d > pd.to_datetime(train_end)) & (d <= pd.to_datetime(valid_end))\n",
        "    test  = d > pd.to_datetime(valid_end)\n",
        "    return train.fillna(False), valid.fillna(False), test.fillna(False), date_col\n",
        "\n",
        "if 'pd_default_flag' in pd_df.columns:\n",
        "    y = pd_df['pd_default_flag'].astype(int)\n",
        "    tr, va, te, split_col = make_splits(pd_df)\n",
        "    print(\"PD split by:\", split_col or \"random\")\n",
        "    print_pd_metrics(y[va], pd_df.loc[va,'PD'].values, f\"PD ({pd_model_name}) VALID\")\n",
        "    print_pd_metrics(y[te], pd_df.loc[te,'PD'].values, f\"PD ({pd_model_name}) TEST \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOv24YDCrctO",
        "outputId": "f166b5d2-eccd-4770-b833-6cce0fb3bdfb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropping extra columns not used in PD training: ['original_combined_loan_to_value_cltv']\n",
            "PD split by: first_payment_date\n",
            "[PD (pd_model_xgb_isotonic.joblib) VALID] AUC=0.7693 | KS=0.4078 | HL=0.00 (p=1.000) | Brier=0.03093\n",
            "[PD (pd_model_xgb_isotonic.joblib) TEST ] AUC=0.7189 | KS=0.3425 | HL=5,385.93 (p=0.000) | Brier=0.02039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1010402527.py:9: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  g = df.groupby(\"bin\").agg(n=(\"y\",\"size\"), obs=(\"y\",\"sum\"), pred=(\"p\",\"sum\")).reset_index(drop=True)\n",
            "/tmp/ipython-input-1010402527.py:9: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  g = df.groupby(\"bin\").agg(n=(\"y\",\"size\"), obs=(\"y\",\"sum\"), pred=(\"p\",\"sum\")).reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_lgd(model_obj, calibrator, X_df):\n",
        "    \"\"\"\n",
        "    model_obj: either a Pipeline (RF/XGB) or dict {'ridge', 'num_pipe'} for logit-OLS\n",
        "    calibrator: IsotonicRegression fitted on VALID (maps raw LGD->calibrated LGD)\n",
        "    \"\"\"\n",
        "    if isinstance(model_obj, dict):  # ridge + scaler\n",
        "        ridge = model_obj['ridge']; num_pipe = model_obj['num_pipe']\n",
        "        Xp = num_pipe.transform(X_df)\n",
        "        # model predicts logit(LGD) → inverse-logit\n",
        "        z = ridge.predict(Xp)\n",
        "        lgd_raw = 1/(1+np.exp(-z))\n",
        "    else:\n",
        "        lgd_raw = model_obj.predict(X_df)\n",
        "    lgd_raw = np.clip(lgd_raw, 0.0, 1.0)\n",
        "    if calibrator is not None:\n",
        "        lgd_cal = calibrator.transform(lgd_raw)\n",
        "        return np.clip(lgd_cal, 0.0, 1.0)\n",
        "    return lgd_raw\n",
        "\n",
        "# choose LGD model to use (default to XGB if present, else RF, else Ridge)\n",
        "LGD_PICK = next((k for k in [\"xgb\",\"rf\",\"ridge\"] if k in lgd_loaded), None)\n",
        "assert LGD_PICK is not None, \"No LGD model files found.\"\n",
        "lgd_model_obj = lgd_loaded[LGD_PICK]\n",
        "lgd_calib     = lgd_cal.get(LGD_PICK, None)\n",
        "\n",
        "pd_df['LGD'] = predict_lgd(lgd_model_obj, lgd_calib, X)\n",
        "print(\"LGD model used:\", LGD_PICK, \"| mean LGD:\", pd_df['LGD'].mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNVsk7c-t623",
        "outputId": "f716aa90-b94c-4cc4-ca76-b80f2f5e28f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGD model used: xgb | mean LGD: 0.96000147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict_ccf(model_obj, X_df, calibrator=None):\n",
        "    \"\"\"Score CCF with the same preprocessing as training, then (optionally) apply isotonic calibrator.\"\"\"\n",
        "    if isinstance(model_obj, dict):  # ridge + scaler bundle\n",
        "        ridge = model_obj['ridge']; num_pipe = model_obj['num_pipe']\n",
        "        Xp = num_pipe.transform(X_df)\n",
        "        ccf_raw = ridge.predict(Xp)\n",
        "    else:  # Pipeline([impute, model]) for RF / XGB\n",
        "        ccf_raw = model_obj.predict(X_df)\n",
        "\n",
        "    ccf_raw = np.clip(ccf_raw, 0.0, 2.0)\n",
        "    if calibrator is not None:\n",
        "        ccf_cal = calibrator.transform(ccf_raw)\n",
        "        return np.clip(ccf_cal, 0.0, 2.0)\n",
        "    return ccf_raw\n",
        "\n",
        "\n",
        "def ead_expected_features(model_obj):\n",
        "    \"\"\"Return the feature list the fitted EAD model expects (from the imputer/scaler).\"\"\"\n",
        "    # dict bundle: use the fitted preprocessing pipeline\n",
        "    if isinstance(model_obj, dict) and 'num_pipe' in model_obj:\n",
        "        pipe = model_obj['num_pipe']\n",
        "        if hasattr(pipe, 'named_steps'):\n",
        "            for step in ['impute', 'scale']:\n",
        "                st = pipe.named_steps.get(step)\n",
        "                if st is not None and hasattr(st, 'feature_names_in_'):\n",
        "                    return list(st.feature_names_in_)\n",
        "        if hasattr(pipe, 'feature_names_in_'):\n",
        "            return list(pipe.feature_names_in_)\n",
        "\n",
        "    # sklearn Pipeline for RF/XGB: imputer step holds names\n",
        "    if hasattr(model_obj, 'named_steps'):\n",
        "        imp = model_obj.named_steps.get('impute')\n",
        "        if imp is not None and hasattr(imp, 'feature_names_in_'):\n",
        "            return list(imp.feature_names_in_)\n",
        "        if hasattr(model_obj, 'feature_names_in_'):\n",
        "            return list(model_obj.feature_names_in_)\n",
        "\n",
        "    # last resort\n",
        "    return None\n",
        "\n",
        "\n",
        "# Build the scoring matrix EXACTLY as the model was trained\n",
        "ead_feat_list = ead_expected_features(ead_loaded.get('xgb') or ead_loaded.get('rf') or ead_loaded.get('ridge'))\n",
        "if ead_feat_list is None:\n",
        "    raise RuntimeError(\"Could not infer EAD model feature list; please keep the imputer/scaler fitted when saving.\")\n",
        "\n",
        "# Make X_ead in the correct order and with the same columns\n",
        "X_ead = pd_df.reindex(columns=ead_feat_list, fill_value=0.0).copy()\n",
        "for c in X_ead.columns:\n",
        "    X_ead[c] = pd.to_numeric(X_ead[c], errors='coerce')\n",
        "\n",
        "# Pick the best available model in your folder (xgb → rf → ridge), and its calibrator\n",
        "EAD_PICK = next((k for k in [\"xgb\",\"rf\",\"ridge\"] if k in ead_loaded), None)\n",
        "assert EAD_PICK is not None, \"No EAD model files found.\"\n",
        "ead_model_obj = ead_loaded[EAD_PICK]\n",
        "ead_calib     = ead_cal.get(EAD_PICK, None)\n",
        "\n",
        "# Compute CCF and EAD\n",
        "pd_df['CCF'] = predict_ccf(ead_model_obj, X_ead, calibrator=ead_calib)\n",
        "\n",
        "# Ensure ORIG_UPB_COL is defined (use the one from earlier cells)\n",
        "if 'ORIG_UPB_COL' not in globals() or ORIG_UPB_COL not in pd_df.columns:\n",
        "    ORIG_UPB_COL = next((c for c in ['original_upb','orig_upb'] if c in pd_df.columns), None)\n",
        "    assert ORIG_UPB_COL is not None, \"Missing original UPB column.\"\n",
        "\n",
        "pd_df['EAD'] = pd.to_numeric(pd_df[ORIG_UPB_COL], errors='coerce') * pd_df['CCF']\n",
        "print(\"EAD model used:\", EAD_PICK, \"| mean CCF:\", pd_df['CCF'].mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLRRKjtpuuxy",
        "outputId": "25fb3c95-850a-4085-bae3-e9e8312a8079"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EAD model used: xgb | mean CCF: 0.9827604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd_df['EL'] = pd_df['PD'] * pd_df['LGD'] * pd_df['EAD']\n",
        "print(\"Portfolio EL ($):\", pd_df['EL'].sum().round(2))\n",
        "\n",
        "def money(x):\n",
        "    return f\"${x:,.0f}\"\n",
        "\n",
        "by_state = pd_df.groupby('property_state', dropna=False)['EL'].sum().sort_values(ascending=False).head(10)\n",
        "by_vintage = pd_df.assign(vintage=pd.to_datetime(pd_df.get('first_payment_date'), errors='coerce').dt.year)\\\n",
        "                  .groupby('vintage')['EL'].sum().sort_values(ascending=False).head(10)\n",
        "by_ltv = pd_df.groupby('ltv_bin', dropna=False)['EL'].sum().sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nTop-10 EL by state:\\n\", by_state.map(lambda x: float(x)).apply(money))\n",
        "print(\"\\nTop-10 EL by vintage:\\n\", by_vintage.map(lambda x: float(x)).apply(money))\n",
        "print(\"\\nEL by LTV bin:\\n\", by_ltv.map(lambda x: float(x)).apply(money))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z78KsBJmy8md",
        "outputId": "63431501-5d64-4fb3-a73a-f637a1cbc40e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Portfolio EL ($): 7639337165.83\n",
            "\n",
            "Top-10 EL by state:\n",
            " property_state\n",
            "CA    $1,119,956,268\n",
            "FL      $843,212,242\n",
            "TX      $677,187,042\n",
            "NY      $550,468,209\n",
            "NJ      $347,280,073\n",
            "IL      $335,231,567\n",
            "GA      $249,682,939\n",
            "MD      $214,074,150\n",
            "PA      $213,831,529\n",
            "WA      $190,162,203\n",
            "Name: EL, dtype: object\n",
            "\n",
            "Top-10 EL by vintage:\n",
            " vintage\n",
            "2023    $983,445,801\n",
            "2024    $909,899,377\n",
            "2022    $712,321,968\n",
            "2019    $540,404,237\n",
            "2018    $534,901,346\n",
            "2017    $522,325,510\n",
            "2016    $459,351,660\n",
            "2015    $426,193,269\n",
            "2014    $425,711,267\n",
            "2020    $396,514,049\n",
            "Name: EL, dtype: object\n",
            "\n",
            "EL by LTV bin:\n",
            " ltv_bin\n",
            "70-80    $2,411,669,265\n",
            "90-95    $1,652,208,096\n",
            "80-90    $1,130,574,077\n",
            "<=60       $882,992,466\n",
            "60-70      $808,252,876\n",
            ">95        $753,640,385\n",
            "Name: EL, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3367231370.py:10: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  by_ltv = pd_df.groupby('ltv_bin', dropna=False)['EL'].sum().sort_values(ascending=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(RNG_SEED)\n",
        "\n",
        "# Fixed severities per loan\n",
        "loss_if_default = (pd_df['LGD'].values * pd_df['EAD'].values).astype('float64')\n",
        "pdv = pd_df['PD'].values.astype('float64')\n",
        "\n",
        "def simulate_portfolio_losses(n_draws=N_DRAWS, chunk=CHUNK_SIZE):\n",
        "    n = len(pdv)\n",
        "    losses = np.zeros(n_draws, dtype='float64')\n",
        "    for start in range(0, n, chunk):\n",
        "        end = min(start+chunk, n)\n",
        "        p = pdv[start:end]\n",
        "        sev = loss_if_default[start:end]\n",
        "        # For memory: simulate in blocks of draws as well\n",
        "        # We do draws in batches of 250 to keep RAM small\n",
        "        B = 250\n",
        "        for d0 in range(0, n_draws, B):\n",
        "            d1 = min(d0+B, n_draws)\n",
        "            u = rng.random((d1-d0, end-start))\n",
        "            default_mat = (u < p)   # boolean\n",
        "            # sum losses per draw\n",
        "            losses[d0:d1] += default_mat @ sev\n",
        "        del u, default_mat\n",
        "        gc.collect()\n",
        "    return losses\n",
        "\n",
        "losses = simulate_portfolio_losses()\n",
        "def var_cvar(samples, alpha=0.99):\n",
        "    # VaR = alpha-quantile; CVaR = mean of tail beyond VaR\n",
        "    q = np.quantile(samples, alpha)\n",
        "    tail = samples[samples >= q]\n",
        "    return float(q), float(tail.mean())\n",
        "\n",
        "for a in (0.95, 0.99):\n",
        "    v, c = var_cvar(losses, alpha=a)\n",
        "    print(f\"{int(a*100)}% VaR = {money(v)} | {int(a*100)}% CVaR ≈ {money(c)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGTqQgCUzZbJ",
        "outputId": "48fbd604-06dc-4e68-b29f-6afd803c21c6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95% VaR = $7,715,388,397 | 95% CVaR ≈ $7,736,647,168\n",
            "99% VaR = $7,748,897,233 | 99% CVaR ≈ $7,769,372,745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_stress(pd_base, lgd_base, ccf_base, s):\n",
        "    pd_st  = np.clip(pd_base * s[\"pd_mult\"], 1e-6, 1-1e-6)\n",
        "    lgd_st = np.clip(lgd_base * s[\"lgd_mult\"], 0.0, 1.0)\n",
        "    ccf_st = np.clip(ccf_base * s[\"ccf_mult\"], 0.0, 2.0)\n",
        "    ead_st = ccf_st * pd_df[ORIG_UPB_COL].values\n",
        "    el_st  = pd_st * lgd_st * ead_st\n",
        "    return pd_st, lgd_st, ead_st, el_st\n",
        "\n",
        "for name, s in SCENARIOS.items():\n",
        "    _, _, _, el = apply_stress(pd_df['PD'].values, pd_df['LGD'].values, pd_df['CCF'].values, s)\n",
        "    print(f\"[{name}] Portfolio EL = {money(el.sum())}\")\n",
        "\n",
        "    # (Optional) simulate portfolio VaR under this scenario (same severities but stressed PD/LGD/EAD)\n",
        "    pdv_st, lgd_st, ead_st, _ = apply_stress(pd_df['PD'].values, pd_df['LGD'].values, pd_df['CCF'].values, s)\n",
        "    loss_if_default_st = (lgd_st * ead_st).astype('float64')\n",
        "\n",
        "    def simulate_stressed(n_draws=N_DRAWS, chunk=CHUNK_SIZE):\n",
        "        n = len(pdv_st); out = np.zeros(n_draws, dtype='float64')\n",
        "        for start in range(0, n, chunk):\n",
        "            end = min(start+chunk, n)\n",
        "            p = pdv_st[start:end]; sev = loss_if_default_st[start:end]\n",
        "            B=250\n",
        "            for d0 in range(0, n_draws, B):\n",
        "                d1 = min(d0+B, n_draws)\n",
        "                u = rng.random((d1-d0, end-start))\n",
        "                out[d0:d1] += (u < p) @ sev\n",
        "        return out\n",
        "\n",
        "    losses_st = simulate_stressed()\n",
        "    for a in (0.95, 0.99):\n",
        "        v, c = var_cvar(losses_st, alpha=a)\n",
        "        print(f\"   {name} {int(a*100)}% VaR = {money(v)} | CVaR ≈ {money(c)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxP11og-zjN9",
        "outputId": "b6250333-cb74-4619-eaa0-ad1e9fce6aca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Baseline] Portfolio EL = $7,639,337,166\n",
            "   Baseline 95% VaR = $7,714,275,829 | CVaR ≈ $7,732,523,341\n",
            "   Baseline 99% VaR = $7,740,825,611 | CVaR ≈ $7,760,336,891\n",
            "[Adverse] Portfolio EL = $12,561,110,585\n",
            "   Adverse 95% VaR = $12,662,909,372 | CVaR ≈ $12,689,002,506\n",
            "   Adverse 99% VaR = $12,703,716,770 | CVaR ≈ $12,728,264,438\n",
            "[Severe] Portfolio EL = $21,851,430,795\n",
            "   Severe 95% VaR = $21,981,807,981 | CVaR ≈ $22,012,356,903\n",
            "   Severe 99% VaR = $22,034,259,594 | CVaR ≈ $22,059,698,882\n"
          ]
        }
      ]
    }
  ]
}