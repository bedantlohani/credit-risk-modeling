{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def clean_and_merge(orig_df, perf_df):\n",
    "    # Standardize key for merging\n",
    "    key = \"Loan Sequence Number\"\n",
    "    orig_df[key] = orig_df[key].astype(str)\n",
    "    perf_df[key] = perf_df[key].astype(str)\n",
    "\n",
    "    # Merge (m:1), inner join\n",
    "    merged = perf_df.merge(orig_df, on=key, how=\"inner\", validate=\"m:1\")\n",
    "\n",
    "    # Snake‑case the column names\n",
    "    merged.columns = (\n",
    "        merged.columns\n",
    "        .str.lower()\n",
    "        .str.replace(r\"[ \\-\\/()]+\", \"_\", regex=True)\n",
    "        .str.strip(\"_\")\n",
    "    )\n",
    "\n",
    "    # Parse and sort by date\n",
    "    if \"monthly_reporting_period\" in merged.columns:\n",
    "        merged[\"monthly_reporting_period\"] = pd.to_datetime(\n",
    "            merged[\"monthly_reporting_period\"], errors=\"coerce\"\n",
    "        )\n",
    "    if \"first_payment_date\" in merged.columns:\n",
    "        merged[\"first_payment_date\"] = pd.to_datetime(\n",
    "            merged[\"first_payment_date\"], format=\"%Y%m\", errors=\"coerce\"\n",
    "        )\n",
    "    merged = merged.sort_values(\n",
    "        [\"loan_sequence_number\", \"monthly_reporting_period\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Numeric casts\n",
    "    for col in (\"current_actual_upb\", \"original_upb\", \"interest_rate\"):\n",
    "        if col in merged.columns:\n",
    "            merged[col] = pd.to_numeric(merged[col], errors=\"coerce\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1999...\n",
      "  Saved merged_1999.parquet (2,507,096 rows)\n",
      "Processing 2000...\n",
      "  Saved merged_2000.parquet (1,439,897 rows)\n",
      "Processing 2001...\n",
      "  Saved merged_2001.parquet (1,960,552 rows)\n",
      "Processing 2002...\n",
      "  Saved merged_2002.parquet (2,488,606 rows)\n",
      "Processing 2003...\n",
      "  Saved merged_2003.parquet (4,040,135 rows)\n",
      "Processing 2004...\n",
      "  Saved merged_2004.parquet (3,959,723 rows)\n",
      "Processing 2005...\n",
      "  Saved merged_2005.parquet (3,856,796 rows)\n",
      "Processing 2006...\n",
      "  Saved merged_2006.parquet (3,184,782 rows)\n",
      "Processing 2007...\n",
      "  Saved merged_2007.parquet (2,989,909 rows)\n",
      "Processing 2008...\n",
      "  Saved merged_2008.parquet (2,437,525 rows)\n",
      "Processing 2009...\n",
      "  Saved merged_2009.parquet (3,122,523 rows)\n",
      "Processing 2010...\n",
      "  Saved merged_2010.parquet (3,390,711 rows)\n",
      "Processing 2011...\n",
      "  Saved merged_2011.parquet (3,548,320 rows)\n",
      "Processing 2012...\n",
      "  Saved merged_2012.parquet (4,352,734 rows)\n",
      "Processing 2013...\n",
      "  Saved merged_2013.parquet (4,027,063 rows)\n",
      "Processing 2014...\n",
      "  Saved merged_2014.parquet (3,254,048 rows)\n",
      "Processing 2015...\n",
      "  Saved merged_2015.parquet (3,283,966 rows)\n",
      "Processing 2016...\n",
      "  Saved merged_2016.parquet (3,158,076 rows)\n",
      "Processing 2017...\n",
      "  Saved merged_2017.parquet (2,604,385 rows)\n",
      "Processing 2018...\n",
      "  Saved merged_2018.parquet (1,902,207 rows)\n",
      "Processing 2019...\n",
      "  Saved merged_2019.parquet (1,706,966 rows)\n",
      "Processing 2020...\n",
      "  Saved merged_2020.parquet (2,048,809 rows)\n",
      "Processing 2021...\n",
      "  Saved merged_2021.parquet (1,913,130 rows)\n",
      "Processing 2022...\n",
      "  Saved merged_2022.parquet (1,400,239 rows)\n",
      "Processing 2023...\n",
      "  Saved merged_2023.parquet (836,347 rows)\n",
      "Processing 2024...\n",
      "  Saved merged_2024.parquet (254,815 rows)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('../data/processed/merged_by_year', exist_ok=True)\n",
    "\n",
    "for year in range(1999, 2025):\n",
    "    o_path = f\"../data/interim/origination_{year}.parquet\"\n",
    "    p_path = f\"../data/interim/performance_{year}.parquet\"\n",
    "    if not os.path.exists(o_path):\n",
    "        print(f\"  Skipping origination_{year}.parquet (not found)\")\n",
    "        continue\n",
    "    if not os.path.exists(p_path):\n",
    "        print(f\"  Skipping performance_{year}.parquet (not found)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {year}...\")\n",
    "    orig = pd.read_parquet(o_path)\n",
    "    perf = pd.read_parquet(p_path)\n",
    "\n",
    "    merged = clean_and_merge(orig, perf)\n",
    "\n",
    "    out_path = f\"../data/processed/merged_by_year/merged_{year}.parquet\"\n",
    "    merged.to_parquet(out_path, index=False)\n",
    "    print(f\"  Saved merged_{year}.parquet ({len(merged):,} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending merged_1999.parquet\n",
      "Appending merged_2000.parquet\n",
      "Appending merged_2001.parquet\n",
      "Appending merged_2002.parquet\n",
      "Appending merged_2003.parquet\n",
      "Appending merged_2004.parquet\n",
      "Appending merged_2005.parquet\n",
      "Appending merged_2006.parquet\n",
      "Appending merged_2007.parquet\n",
      "Appending merged_2008.parquet\n",
      "Appending merged_2009.parquet\n",
      "Appending merged_2010.parquet\n",
      "Appending merged_2011.parquet\n",
      "Appending merged_2012.parquet\n",
      "Appending merged_2013.parquet\n",
      "Appending merged_2014.parquet\n",
      "Appending merged_2015.parquet\n",
      "Appending merged_2016.parquet\n",
      "Appending merged_2017.parquet\n",
      "Appending merged_2018.parquet\n",
      "Appending merged_2019.parquet\n",
      "Appending merged_2020.parquet\n",
      "Appending merged_2021.parquet\n",
      "Appending merged_2022.parquet\n",
      "Appending merged_2023.parquet\n",
      "Appending merged_2024.parquet\n",
      " Final merged file saved to ../data/processed/merged_loan_performance.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, glob\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "yearly_dir = \"../data/processed/merged_by_year\"\n",
    "final_path = \"../data/processed/merged_loan_performance.parquet\"\n",
    "\n",
    "# 1) Find all per‑year files\n",
    "files = sorted(glob.glob(os.path.join(yearly_dir, \"merged_*.parquet\")))\n",
    "if not files:\n",
    "    raise RuntimeError(f\"No files found in {yearly_dir}\")\n",
    "\n",
    "# 2) Build a \"master\" schema, converting any NullType → string\n",
    "base_schema = pq.read_schema(files[0])\n",
    "fields = []\n",
    "for fld in base_schema:\n",
    "    if pa.types.is_null(fld.type):\n",
    "        fields.append(pa.field(fld.name, pa.string()))\n",
    "    else:\n",
    "        fields.append(fld)\n",
    "master_schema = pa.schema(fields)\n",
    "\n",
    "# 3) Open writer\n",
    "os.makedirs(os.path.dirname(final_path), exist_ok=True)\n",
    "writer = pq.ParquetWriter(final_path, master_schema)\n",
    "\n",
    "for path in files:\n",
    "    print(\"Appending\", os.path.basename(path))\n",
    "    tbl = pq.read_table(path)\n",
    "\n",
    "    # 3a) Make sure every master column exists\n",
    "    for fld in master_schema:\n",
    "        if fld.name not in tbl.schema.names:\n",
    "            null_col = pa.array([None] * tbl.num_rows, type=fld.type)\n",
    "            tbl = tbl.append_column(fld.name, null_col)\n",
    "\n",
    "    # 3b) Build arrays in master order, casting each column into the right type\n",
    "    arrays = []\n",
    "    for fld in master_schema:\n",
    "        col = tbl.column(fld.name)\n",
    "        # if type differs, cast to fld.type\n",
    "        if not col.type.equals(fld.type):\n",
    "            col = col.cast(fld.type)\n",
    "        arrays.append(col)\n",
    "    tbl2 = pa.Table.from_arrays(arrays, names=master_schema.names)\n",
    "\n",
    "    # 3c) Write it out\n",
    "    writer.write_table(tbl2)\n",
    "\n",
    "writer.close()\n",
    "print(\" Final merged file saved to\", final_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataWrangling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
