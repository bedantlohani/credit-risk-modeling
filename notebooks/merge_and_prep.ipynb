{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def clean_and_merge(orig_df, perf_df):\n",
    "    # Standardize key for merging\n",
    "    key = \"Loan Sequence Number\"\n",
    "    orig_df[key] = orig_df[key].astype(str)\n",
    "    perf_df[key] = perf_df[key].astype(str)\n",
    "\n",
    "    # Merge (m:1), inner join\n",
    "    merged = perf_df.merge(orig_df, on=key, how=\"inner\", validate=\"m:1\")\n",
    "\n",
    "    # Snake‑case the column names\n",
    "    merged.columns = (\n",
    "        merged.columns\n",
    "        .str.lower()\n",
    "        .str.replace(r\"[ \\-\\/()]+\", \"_\", regex=True)\n",
    "        .str.strip(\"_\")\n",
    "    )\n",
    "\n",
    "    # Parse and sort by date\n",
    "    if \"monthly_reporting_period\" in merged.columns:\n",
    "        merged[\"monthly_reporting_period\"] = pd.to_datetime(\n",
    "            merged[\"monthly_reporting_period\"], errors=\"coerce\"\n",
    "        )\n",
    "    if \"first_payment_date\" in merged.columns:\n",
    "        merged[\"first_payment_date\"] = pd.to_datetime(\n",
    "            merged[\"first_payment_date\"], format=\"%Y%m\", errors=\"coerce\"\n",
    "        )\n",
    "    merged = merged.sort_values(\n",
    "        [\"loan_sequence_number\", \"monthly_reporting_period\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Numeric casts\n",
    "    for col in (\"current_actual_upb\", \"original_upb\", \"interest_rate\"):\n",
    "        if col in merged.columns:\n",
    "            merged[col] = pd.to_numeric(merged[col], errors=\"coerce\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1999...\n",
      "  Saved merged_1999.parquet (2,507,096 rows)\n",
      "Processing 2000...\n",
      "  Saved merged_2000.parquet (1,439,897 rows)\n",
      "Processing 2001...\n",
      "  Saved merged_2001.parquet (1,960,552 rows)\n",
      "Processing 2002...\n",
      "  Saved merged_2002.parquet (2,488,606 rows)\n",
      "Processing 2003...\n",
      "  Saved merged_2003.parquet (4,040,135 rows)\n",
      "Processing 2004...\n",
      "  Saved merged_2004.parquet (3,959,723 rows)\n",
      "Processing 2005...\n",
      "  Saved merged_2005.parquet (3,856,796 rows)\n",
      "Processing 2006...\n",
      "  Saved merged_2006.parquet (3,184,782 rows)\n",
      "Processing 2007...\n",
      "  Saved merged_2007.parquet (2,989,909 rows)\n",
      "Processing 2008...\n",
      "  Saved merged_2008.parquet (2,437,525 rows)\n",
      "Processing 2009...\n",
      "  Saved merged_2009.parquet (3,122,523 rows)\n",
      "Processing 2010...\n",
      "  Saved merged_2010.parquet (3,390,711 rows)\n",
      "Processing 2011...\n",
      "  Saved merged_2011.parquet (3,548,320 rows)\n",
      "Processing 2012...\n",
      "  Saved merged_2012.parquet (4,352,734 rows)\n",
      "Processing 2013...\n",
      "  Saved merged_2013.parquet (4,027,063 rows)\n",
      "Processing 2014...\n",
      "  Saved merged_2014.parquet (3,254,048 rows)\n",
      "Processing 2015...\n",
      "  Saved merged_2015.parquet (3,283,966 rows)\n",
      "Processing 2016...\n",
      "  Saved merged_2016.parquet (3,158,076 rows)\n",
      "Processing 2017...\n",
      "  Saved merged_2017.parquet (2,604,385 rows)\n",
      "Processing 2018...\n",
      "  Saved merged_2018.parquet (1,902,207 rows)\n",
      "Processing 2019...\n",
      "  Saved merged_2019.parquet (1,706,966 rows)\n",
      "Processing 2020...\n",
      "  Saved merged_2020.parquet (2,048,809 rows)\n",
      "Processing 2021...\n",
      "  Saved merged_2021.parquet (1,913,130 rows)\n",
      "Processing 2022...\n",
      "  Saved merged_2022.parquet (1,400,239 rows)\n",
      "Processing 2023...\n",
      "  Saved merged_2023.parquet (836,347 rows)\n",
      "Processing 2024...\n",
      "  Saved merged_2024.parquet (254,815 rows)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('../data/processed/merged_by_year', exist_ok=True)\n",
    "\n",
    "for year in range(2010, 2025):\n",
    "    o_path = f\"../data/interim/origination_{year}.parquet\"\n",
    "    p_path = f\"../data/interim/performance_{year}.parquet\"\n",
    "    if not os.path.exists(o_path):\n",
    "        print(f\"  Skipping origination_{year}.parquet (not found)\")\n",
    "        continue\n",
    "    if not os.path.exists(p_path):\n",
    "        print(f\"  Skipping performance_{year}.parquet (not found)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {year}...\")\n",
    "    orig = pd.read_parquet(o_path)\n",
    "    perf = pd.read_parquet(p_path)\n",
    "\n",
    "    merged = clean_and_merge(orig, perf)\n",
    "\n",
    "    out_path = f\"../data/processed/merged_by_year/merged_{year}.parquet\"\n",
    "    merged.to_parquet(out_path, index=False)\n",
    "    print(f\"  Saved merged_{year}.parquet ({len(merged):,} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will merge these files (2010–2024):\n",
      " - merged_2010.parquet\n",
      " - merged_2011.parquet\n",
      " - merged_2012.parquet\n",
      " - merged_2013.parquet\n",
      " - merged_2014.parquet\n",
      " - merged_2015.parquet\n",
      " - merged_2016.parquet\n",
      " - merged_2017.parquet\n",
      " - merged_2018.parquet\n",
      " - merged_2019.parquet\n",
      " - merged_2020.parquet\n",
      " - merged_2021.parquet\n",
      " - merged_2022.parquet\n",
      " - merged_2023.parquet\n",
      " - merged_2024.parquet\n",
      "\n",
      "Master schema columns: 63\n",
      "Appending merged_2010.parquet\n",
      "Appending merged_2011.parquet\n",
      "Appending merged_2012.parquet\n",
      "Appending merged_2013.parquet\n",
      "Appending merged_2014.parquet\n",
      "Appending merged_2015.parquet\n",
      "Appending merged_2016.parquet\n",
      "Appending merged_2017.parquet\n",
      "Appending merged_2018.parquet\n",
      "Appending merged_2019.parquet\n",
      "Appending merged_2020.parquet\n",
      "Appending merged_2021.parquet\n",
      "Appending merged_2022.parquet\n",
      "Appending merged_2023.parquet\n",
      "Appending merged_2024.parquet\n",
      "\n",
      "Final merged file saved to ../data/processed/merged_loan_performance_2010_2024.parquet\n"
     ]
    }
   ],
   "source": [
    "import os, glob, re\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc  # for safe=False casting fallback\n",
    "\n",
    "yearly_dir = \"../data/processed/merged_by_year\"\n",
    "final_path = \"../data/processed/merged_loan_performance_2010_2024.parquet\"\n",
    "\n",
    "# ---------- 1) Collect only 2010–2024 files ----------\n",
    "pattern = os.path.join(yearly_dir, \"merged_*.parquet\")\n",
    "\n",
    "def year_from_path(p: str):\n",
    "    m = re.search(r\"(\\d{4})\", os.path.basename(p))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "all_files = sorted(glob.glob(pattern))\n",
    "files = [p for p in all_files if (year_from_path(p) is not None and 2010 <= year_from_path(p) <= 2024)]\n",
    "\n",
    "if not files:\n",
    "    raise RuntimeError(f\"No 2010–2024 files found in {yearly_dir}. Saw: {[os.path.basename(x) for x in all_files]}\")\n",
    "\n",
    "print(\"Will merge these files (2010–2024):\")\n",
    "for p in files:\n",
    "    print(\" -\", os.path.basename(p))\n",
    "\n",
    "# ---------- 2) Build master schema (start from first, then include any new columns that appear later) ----------\n",
    "base_schema = pq.read_schema(files[0])\n",
    "\n",
    "def sanitize_field(fld: pa.Field) -> pa.Field:\n",
    "    # Convert NullType columns to string so they can be casted later\n",
    "    t = fld.type\n",
    "    if pa.types.is_null(t):\n",
    "        t = pa.string()\n",
    "    return pa.field(fld.name, t)\n",
    "\n",
    "fields = [sanitize_field(f) for f in base_schema]\n",
    "master_schema = pa.schema(fields)\n",
    "\n",
    "# Add any columns that show up in later files but aren't in the base schema\n",
    "for path in files[1:]:\n",
    "    sch = pq.read_schema(path)\n",
    "    for fld in sch:\n",
    "        if fld.name not in master_schema.names:\n",
    "            master_schema = master_schema.append(sanitize_field(fld))\n",
    "\n",
    "print(\"\\nMaster schema columns:\", len(master_schema.names))\n",
    "\n",
    "# ---------- 3) Open writer and append year-by-year ----------\n",
    "os.makedirs(os.path.dirname(final_path), exist_ok=True)\n",
    "writer = pq.ParquetWriter(final_path, master_schema, compression=\"snappy\")\n",
    "\n",
    "for path in files:\n",
    "    print(\"Appending\", os.path.basename(path))\n",
    "    tbl = pq.read_table(path)\n",
    "\n",
    "    # 3a) Ensure every master column exists; if missing, add a null column of the right type\n",
    "    for fld in master_schema:\n",
    "        if fld.name not in tbl.schema.names:\n",
    "            tbl = tbl.append_column(fld.name, pa.nulls(tbl.num_rows, type=fld.type))\n",
    "\n",
    "    # 3b) Reorder and cast columns to master types\n",
    "    cols = []\n",
    "    for fld in master_schema:\n",
    "        col = tbl.column(fld.name)\n",
    "        if not col.type.equals(fld.type):\n",
    "            try:\n",
    "                col = col.cast(fld.type)  # safe cast\n",
    "            except pa.ArrowInvalid:\n",
    "                # last resort: unsafe cast (e.g., int->string)\n",
    "                col = pc.cast(col, target_type=fld.type, safe=False)\n",
    "        cols.append(col)\n",
    "\n",
    "    tbl2 = pa.Table.from_arrays(cols, names=master_schema.names)\n",
    "\n",
    "    # 3c) Write this chunk (row_group_size helps keep memory predictable)\n",
    "    writer.write_table(tbl2, row_group_size=1_000_000)\n",
    "\n",
    "writer.close()\n",
    "print(\"\\nFinal merged file saved to\", final_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataWrangling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
